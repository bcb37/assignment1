{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD8CAYAAACGsIhGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKBQhUcpCoVAoFCH5fxzuAEi311imAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b073240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "N = 100 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
    "y = np.zeros(N*K, dtype='uint8') # class labels\n",
    "r = np.linspace(0.0,1,N) # radius\n",
    "for j in range(K):\n",
    "  ix = range(N*j,N*(j+1))\n",
    "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "  X[ix] = np.c_[r*np.cos(t), r*np.sin(t)]\n",
    "  y[ix] = j\n",
    "# lets visualize the data:\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = 0.01 * np.random.randn(D,K)\n",
    "b = np.zeros((1,K))\n",
    "step_size = 1e-0\n",
    "reg = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 1.065487\n",
      "iteration 10: loss 0.890768\n",
      "iteration 20: loss 0.825876\n",
      "iteration 30: loss 0.796658\n",
      "iteration 40: loss 0.781637\n",
      "iteration 50: loss 0.773198\n",
      "iteration 60: loss 0.768154\n",
      "iteration 70: loss 0.765001\n",
      "iteration 80: loss 0.762962\n",
      "iteration 90: loss 0.761609\n",
      "iteration 100: loss 0.760692\n",
      "iteration 110: loss 0.760061\n",
      "iteration 120: loss 0.759620\n",
      "iteration 130: loss 0.759310\n",
      "iteration 140: loss 0.759089\n",
      "iteration 150: loss 0.758930\n",
      "iteration 160: loss 0.758816\n",
      "iteration 170: loss 0.758733\n",
      "iteration 180: loss 0.758673\n",
      "iteration 190: loss 0.758629\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    scores = np.dot(X, W) + b\n",
    "    num_examples = X.shape[0]\n",
    "    # get unnormalized probabilities\n",
    "    exp_scores = np.exp(scores)\n",
    "    # normalize them for each example\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "    # compute the loss: average cross-entropy loss and regularization\n",
    "\n",
    "    data_loss = np.sum(corect_logprobs)/num_examples\n",
    "    reg_loss = 0.5*reg*np.sum(W*W)\n",
    "    loss = data_loss + reg_loss\n",
    "    if i % 10 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, loss))\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),y] -= 1\n",
    "    dscores /= num_examples\n",
    "    dW = np.dot(X.T, dscores)\n",
    "    db = np.sum(dscores, axis=0, keepdims=True)\n",
    "    dW += reg*W # don't forget the regularization gradient\n",
    "\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.52\n"
     ]
    }
   ],
   "source": [
    "# evaluate training set accuracy\n",
    "scores = np.dot(X, W) + b\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print('training accuracy: %.2f' % (np.mean(predicted_class == y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize parameters randomly\n",
    "h = 100 # size of hidden layer\n",
    "W1 = 0.01 * np.random.randn(D,h)\n",
    "b1 = np.zeros((1,h))\n",
    "W2 = 0.01 * np.random.randn(h,K)\n",
    "b2 = np.zeros((1,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 1.098643\n",
      "iteration 10: loss 1.094128\n",
      "iteration 20: loss 1.066497\n",
      "iteration 30: loss 0.941608\n",
      "iteration 40: loss 0.789456\n",
      "iteration 50: loss 0.735766\n",
      "iteration 60: loss 0.719614\n",
      "iteration 70: loss 0.711437\n",
      "iteration 80: loss 0.705666\n",
      "iteration 90: loss 0.700726\n",
      "iteration 100: loss 0.696080\n",
      "iteration 110: loss 0.691531\n",
      "iteration 120: loss 0.686945\n",
      "iteration 130: loss 0.682434\n",
      "iteration 140: loss 0.680916\n",
      "iteration 150: loss 0.742077\n",
      "iteration 160: loss 0.714364\n",
      "iteration 170: loss 0.704382\n",
      "iteration 180: loss 0.713535\n",
      "iteration 190: loss 0.706080\n",
      "iteration 200: loss 0.704467\n",
      "iteration 210: loss 0.700949\n",
      "iteration 220: loss 0.695252\n",
      "iteration 230: loss 0.693506\n",
      "iteration 240: loss 0.740248\n",
      "iteration 250: loss 0.702240\n",
      "iteration 260: loss 0.680221\n",
      "iteration 270: loss 0.650848\n",
      "iteration 280: loss 0.675601\n",
      "iteration 290: loss 0.657266\n",
      "iteration 300: loss 0.618553\n",
      "iteration 310: loss 0.592095\n",
      "iteration 320: loss 0.612205\n",
      "iteration 330: loss 0.588264\n",
      "iteration 340: loss 0.570608\n",
      "iteration 350: loss 0.531575\n",
      "iteration 360: loss 0.503909\n",
      "iteration 370: loss 0.561499\n",
      "iteration 380: loss 0.496173\n",
      "iteration 390: loss 0.457729\n",
      "iteration 400: loss 0.476930\n",
      "iteration 410: loss 0.460371\n",
      "iteration 420: loss 0.507668\n",
      "iteration 430: loss 0.414168\n",
      "iteration 440: loss 0.388905\n",
      "iteration 450: loss 0.477791\n",
      "iteration 460: loss 0.408503\n",
      "iteration 470: loss 0.380044\n",
      "iteration 480: loss 0.381689\n",
      "iteration 490: loss 0.383170\n",
      "iteration 500: loss 0.370016\n",
      "iteration 510: loss 0.365478\n",
      "iteration 520: loss 0.415627\n",
      "iteration 530: loss 0.368536\n",
      "iteration 540: loss 0.334441\n",
      "iteration 550: loss 0.331909\n",
      "iteration 560: loss 0.340158\n",
      "iteration 570: loss 0.358241\n",
      "iteration 580: loss 0.344722\n",
      "iteration 590: loss 0.337144\n",
      "iteration 600: loss 0.344186\n",
      "iteration 610: loss 0.353623\n",
      "iteration 620: loss 0.337273\n",
      "iteration 630: loss 0.320660\n",
      "iteration 640: loss 0.316815\n",
      "iteration 650: loss 0.325200\n",
      "iteration 660: loss 0.372515\n",
      "iteration 670: loss 0.324562\n",
      "iteration 680: loss 0.312500\n",
      "iteration 690: loss 0.321661\n",
      "iteration 700: loss 0.339713\n",
      "iteration 710: loss 0.321627\n",
      "iteration 720: loss 0.310138\n",
      "iteration 730: loss 0.308392\n",
      "iteration 740: loss 0.316830\n",
      "iteration 750: loss 0.419169\n",
      "iteration 760: loss 0.312430\n",
      "iteration 770: loss 0.300522\n",
      "iteration 780: loss 0.297798\n",
      "iteration 790: loss 0.296341\n",
      "iteration 800: loss 0.295535\n",
      "iteration 810: loss 0.295423\n",
      "iteration 820: loss 0.297769\n",
      "iteration 830: loss 0.308320\n",
      "iteration 840: loss 0.319313\n",
      "iteration 850: loss 0.304176\n",
      "iteration 860: loss 0.296158\n",
      "iteration 870: loss 0.295567\n",
      "iteration 880: loss 0.330942\n",
      "iteration 890: loss 0.460291\n",
      "iteration 900: loss 0.298592\n",
      "iteration 910: loss 0.293652\n",
      "iteration 920: loss 0.291069\n",
      "iteration 930: loss 0.289710\n",
      "iteration 940: loss 0.288824\n",
      "iteration 950: loss 0.288190\n",
      "iteration 960: loss 0.287689\n",
      "iteration 970: loss 0.287539\n",
      "iteration 980: loss 0.289005\n",
      "iteration 990: loss 0.300896\n",
      "iteration 1000: loss 0.395608\n",
      "iteration 1010: loss 0.305363\n",
      "iteration 1020: loss 0.288044\n",
      "iteration 1030: loss 0.286083\n",
      "iteration 1040: loss 0.285073\n",
      "iteration 1050: loss 0.284433\n",
      "iteration 1060: loss 0.283920\n",
      "iteration 1070: loss 0.283731\n",
      "iteration 1080: loss 0.285421\n",
      "iteration 1090: loss 0.294052\n",
      "iteration 1100: loss 0.327409\n",
      "iteration 1110: loss 0.328887\n",
      "iteration 1120: loss 0.285950\n",
      "iteration 1130: loss 0.282413\n",
      "iteration 1140: loss 0.281681\n",
      "iteration 1150: loss 0.282079\n",
      "iteration 1160: loss 0.285232\n",
      "iteration 1170: loss 0.298207\n",
      "iteration 1180: loss 0.325879\n",
      "iteration 1190: loss 0.296820\n",
      "iteration 1200: loss 0.281317\n",
      "iteration 1210: loss 0.279709\n",
      "iteration 1220: loss 0.279878\n",
      "iteration 1230: loss 0.281999\n",
      "iteration 1240: loss 0.289053\n",
      "iteration 1250: loss 0.305193\n",
      "iteration 1260: loss 0.298011\n",
      "iteration 1270: loss 0.281621\n",
      "iteration 1280: loss 0.278692\n",
      "iteration 1290: loss 0.278919\n",
      "iteration 1300: loss 0.280877\n",
      "iteration 1310: loss 0.286187\n",
      "iteration 1320: loss 0.293664\n",
      "iteration 1330: loss 0.289220\n",
      "iteration 1340: loss 0.280169\n",
      "iteration 1350: loss 0.278401\n",
      "iteration 1360: loss 0.278379\n",
      "iteration 1370: loss 0.279940\n",
      "iteration 1380: loss 0.283252\n",
      "iteration 1390: loss 0.286128\n",
      "iteration 1400: loss 0.285059\n",
      "iteration 1410: loss 0.280610\n",
      "iteration 1420: loss 0.278203\n",
      "iteration 1430: loss 0.277299\n",
      "iteration 1440: loss 0.277006\n",
      "iteration 1450: loss 0.277808\n",
      "iteration 1460: loss 0.278866\n",
      "iteration 1470: loss 0.279082\n",
      "iteration 1480: loss 0.278176\n",
      "iteration 1490: loss 0.276657\n",
      "iteration 1500: loss 0.275321\n",
      "iteration 1510: loss 0.274136\n",
      "iteration 1520: loss 0.273498\n",
      "iteration 1530: loss 0.273388\n",
      "iteration 1540: loss 0.273225\n",
      "iteration 1550: loss 0.273517\n",
      "iteration 1560: loss 0.273543\n",
      "iteration 1570: loss 0.273549\n",
      "iteration 1580: loss 0.273324\n",
      "iteration 1590: loss 0.273002\n",
      "iteration 1600: loss 0.272139\n",
      "iteration 1610: loss 0.271532\n",
      "iteration 1620: loss 0.270810\n",
      "iteration 1630: loss 0.270208\n",
      "iteration 1640: loss 0.269023\n",
      "iteration 1650: loss 0.268776\n",
      "iteration 1660: loss 0.269037\n",
      "iteration 1670: loss 0.269772\n",
      "iteration 1680: loss 0.272637\n",
      "iteration 1690: loss 0.277740\n",
      "iteration 1700: loss 0.278811\n",
      "iteration 1710: loss 0.273278\n",
      "iteration 1720: loss 0.269127\n",
      "iteration 1730: loss 0.266724\n",
      "iteration 1740: loss 0.265590\n",
      "iteration 1750: loss 0.264891\n",
      "iteration 1760: loss 0.264844\n",
      "iteration 1770: loss 0.265249\n",
      "iteration 1780: loss 0.266726\n",
      "iteration 1790: loss 0.269860\n",
      "iteration 1800: loss 0.273381\n",
      "iteration 1810: loss 0.273937\n",
      "iteration 1820: loss 0.270263\n",
      "iteration 1830: loss 0.266613\n",
      "iteration 1840: loss 0.264631\n",
      "iteration 1850: loss 0.263720\n",
      "iteration 1860: loss 0.263431\n",
      "iteration 1870: loss 0.263556\n",
      "iteration 1880: loss 0.264067\n",
      "iteration 1890: loss 0.265381\n",
      "iteration 1900: loss 0.266834\n",
      "iteration 1910: loss 0.268138\n",
      "iteration 1920: loss 0.267972\n",
      "iteration 1930: loss 0.266676\n",
      "iteration 1940: loss 0.265110\n",
      "iteration 1950: loss 0.263991\n",
      "iteration 1960: loss 0.263489\n",
      "iteration 1970: loss 0.263317\n",
      "iteration 1980: loss 0.263466\n",
      "iteration 1990: loss 0.263666\n",
      "iteration 2000: loss 0.264021\n",
      "iteration 2010: loss 0.264211\n",
      "iteration 2020: loss 0.264288\n",
      "iteration 2030: loss 0.264256\n",
      "iteration 2040: loss 0.264031\n",
      "iteration 2050: loss 0.263722\n",
      "iteration 2060: loss 0.263257\n",
      "iteration 2070: loss 0.262889\n",
      "iteration 2080: loss 0.262705\n",
      "iteration 2090: loss 0.262717\n",
      "iteration 2100: loss 0.262557\n",
      "iteration 2110: loss 0.262677\n",
      "iteration 2120: loss 0.262734\n",
      "iteration 2130: loss 0.262721\n",
      "iteration 2140: loss 0.262620\n",
      "iteration 2150: loss 0.262590\n",
      "iteration 2160: loss 0.262485\n",
      "iteration 2170: loss 0.262128\n",
      "iteration 2180: loss 0.261941\n",
      "iteration 2190: loss 0.261896\n",
      "iteration 2200: loss 0.261742\n",
      "iteration 2210: loss 0.261809\n",
      "iteration 2220: loss 0.261728\n",
      "iteration 2230: loss 0.261728\n",
      "iteration 2240: loss 0.261322\n",
      "iteration 2250: loss 0.261224\n",
      "iteration 2260: loss 0.261143\n",
      "iteration 2270: loss 0.260997\n",
      "iteration 2280: loss 0.260981\n",
      "iteration 2290: loss 0.261100\n",
      "iteration 2300: loss 0.261320\n",
      "iteration 2310: loss 0.261306\n",
      "iteration 2320: loss 0.261539\n",
      "iteration 2330: loss 0.261547\n",
      "iteration 2340: loss 0.261344\n",
      "iteration 2350: loss 0.261171\n",
      "iteration 2360: loss 0.260965\n",
      "iteration 2370: loss 0.260657\n",
      "iteration 2380: loss 0.260334\n",
      "iteration 2390: loss 0.260124\n",
      "iteration 2400: loss 0.259726\n",
      "iteration 2410: loss 0.259906\n",
      "iteration 2420: loss 0.259903\n",
      "iteration 2430: loss 0.260023\n",
      "iteration 2440: loss 0.260211\n",
      "iteration 2450: loss 0.260322\n",
      "iteration 2460: loss 0.260251\n",
      "iteration 2470: loss 0.260216\n",
      "iteration 2480: loss 0.260075\n",
      "iteration 2490: loss 0.260047\n",
      "iteration 2500: loss 0.259931\n",
      "iteration 2510: loss 0.259802\n",
      "iteration 2520: loss 0.259675\n",
      "iteration 2530: loss 0.259580\n",
      "iteration 2540: loss 0.259515\n",
      "iteration 2550: loss 0.259451\n",
      "iteration 2560: loss 0.259379\n",
      "iteration 2570: loss 0.259306\n",
      "iteration 2580: loss 0.259264\n",
      "iteration 2590: loss 0.259134\n",
      "iteration 2600: loss 0.258966\n",
      "iteration 2610: loss 0.258864\n",
      "iteration 2620: loss 0.258619\n",
      "iteration 2630: loss 0.258556\n",
      "iteration 2640: loss 0.258490\n",
      "iteration 2650: loss 0.258337\n",
      "iteration 2660: loss 0.258212\n",
      "iteration 2670: loss 0.258150\n",
      "iteration 2680: loss 0.258110\n",
      "iteration 2690: loss 0.258098\n",
      "iteration 2700: loss 0.258100\n",
      "iteration 2710: loss 0.258099\n",
      "iteration 2720: loss 0.257987\n",
      "iteration 2730: loss 0.257986\n",
      "iteration 2740: loss 0.257926\n",
      "iteration 2750: loss 0.257782\n",
      "iteration 2760: loss 0.257073\n",
      "iteration 2770: loss 0.256222\n",
      "iteration 2780: loss 0.255504\n",
      "iteration 2790: loss 0.255164\n",
      "iteration 2800: loss 0.255018\n",
      "iteration 2810: loss 0.254876\n",
      "iteration 2820: loss 0.254865\n",
      "iteration 2830: loss 0.254812\n",
      "iteration 2840: loss 0.254851\n",
      "iteration 2850: loss 0.254940\n",
      "iteration 2860: loss 0.255011\n",
      "iteration 2870: loss 0.255147\n",
      "iteration 2880: loss 0.255364\n",
      "iteration 2890: loss 0.255903\n",
      "iteration 2900: loss 0.258070\n",
      "iteration 2910: loss 0.261514\n",
      "iteration 2920: loss 0.265275\n",
      "iteration 2930: loss 0.266137\n",
      "iteration 2940: loss 0.263165\n",
      "iteration 2950: loss 0.259606\n",
      "iteration 2960: loss 0.257182\n",
      "iteration 2970: loss 0.256079\n",
      "iteration 2980: loss 0.255656\n",
      "iteration 2990: loss 0.255446\n",
      "iteration 3000: loss 0.255489\n",
      "iteration 3010: loss 0.255617\n",
      "iteration 3020: loss 0.255805\n",
      "iteration 3030: loss 0.256029\n",
      "iteration 3040: loss 0.256261\n",
      "iteration 3050: loss 0.256475\n",
      "iteration 3060: loss 0.256725\n",
      "iteration 3070: loss 0.256922\n",
      "iteration 3080: loss 0.257006\n",
      "iteration 3090: loss 0.257082\n",
      "iteration 3100: loss 0.257062\n",
      "iteration 3110: loss 0.257092\n",
      "iteration 3120: loss 0.257118\n",
      "iteration 3130: loss 0.257078\n",
      "iteration 3140: loss 0.257053\n",
      "iteration 3150: loss 0.256944\n",
      "iteration 3160: loss 0.256960\n",
      "iteration 3170: loss 0.256896\n",
      "iteration 3180: loss 0.256948\n",
      "iteration 3190: loss 0.256978\n",
      "iteration 3200: loss 0.256974\n",
      "iteration 3210: loss 0.257008\n",
      "iteration 3220: loss 0.256974\n",
      "iteration 3230: loss 0.257014\n",
      "iteration 3240: loss 0.256734\n",
      "iteration 3250: loss 0.256593\n",
      "iteration 3260: loss 0.256343\n",
      "iteration 3270: loss 0.256045\n",
      "iteration 3280: loss 0.255809\n",
      "iteration 3290: loss 0.255777\n",
      "iteration 3300: loss 0.255799\n",
      "iteration 3310: loss 0.255850\n",
      "iteration 3320: loss 0.255905\n",
      "iteration 3330: loss 0.256010\n",
      "iteration 3340: loss 0.256015\n",
      "iteration 3350: loss 0.256022\n",
      "iteration 3360: loss 0.256077\n",
      "iteration 3370: loss 0.256020\n",
      "iteration 3380: loss 0.255998\n",
      "iteration 3390: loss 0.256019\n",
      "iteration 3400: loss 0.255920\n",
      "iteration 3410: loss 0.255882\n",
      "iteration 3420: loss 0.255828\n",
      "iteration 3430: loss 0.255780\n",
      "iteration 3440: loss 0.255792\n",
      "iteration 3450: loss 0.255736\n",
      "iteration 3460: loss 0.255815\n",
      "iteration 3470: loss 0.255881\n",
      "iteration 3480: loss 0.255851\n",
      "iteration 3490: loss 0.255744\n",
      "iteration 3500: loss 0.255623\n",
      "iteration 3510: loss 0.255446\n",
      "iteration 3520: loss 0.255294\n",
      "iteration 3530: loss 0.255321\n",
      "iteration 3540: loss 0.255321\n",
      "iteration 3550: loss 0.255379\n",
      "iteration 3560: loss 0.255398\n",
      "iteration 3570: loss 0.255295\n",
      "iteration 3580: loss 0.255191\n",
      "iteration 3590: loss 0.255156\n",
      "iteration 3600: loss 0.254975\n",
      "iteration 3610: loss 0.255032\n",
      "iteration 3620: loss 0.255212\n",
      "iteration 3630: loss 0.255264\n",
      "iteration 3640: loss 0.255246\n",
      "iteration 3650: loss 0.255167\n",
      "iteration 3660: loss 0.255080\n",
      "iteration 3670: loss 0.255051\n",
      "iteration 3680: loss 0.255006\n",
      "iteration 3690: loss 0.255057\n",
      "iteration 3700: loss 0.254818\n",
      "iteration 3710: loss 0.254777\n",
      "iteration 3720: loss 0.254542\n",
      "iteration 3730: loss 0.254249\n",
      "iteration 3740: loss 0.253634\n",
      "iteration 3750: loss 0.253565\n",
      "iteration 3760: loss 0.253326\n",
      "iteration 3770: loss 0.252755\n",
      "iteration 3780: loss 0.252492\n",
      "iteration 3790: loss 0.252336\n",
      "iteration 3800: loss 0.252238\n",
      "iteration 3810: loss 0.252175\n",
      "iteration 3820: loss 0.252125\n",
      "iteration 3830: loss 0.252087\n",
      "iteration 3840: loss 0.252057\n",
      "iteration 3850: loss 0.252024\n",
      "iteration 3860: loss 0.252023\n",
      "iteration 3870: loss 0.251998\n",
      "iteration 3880: loss 0.252099\n",
      "iteration 3890: loss 0.252575\n",
      "iteration 3900: loss 0.253570\n",
      "iteration 3910: loss 0.255124\n",
      "iteration 3920: loss 0.256641\n",
      "iteration 3930: loss 0.258902\n",
      "iteration 3940: loss 0.260508\n",
      "iteration 3950: loss 0.259642\n",
      "iteration 3960: loss 0.257258\n",
      "iteration 3970: loss 0.254661\n",
      "iteration 3980: loss 0.253302\n",
      "iteration 3990: loss 0.252558\n",
      "iteration 4000: loss 0.252341\n",
      "iteration 4010: loss 0.252113\n",
      "iteration 4020: loss 0.252036\n",
      "iteration 4030: loss 0.251961\n",
      "iteration 4040: loss 0.252022\n",
      "iteration 4050: loss 0.252534\n",
      "iteration 4060: loss 0.253049\n",
      "iteration 4070: loss 0.253549\n",
      "iteration 4080: loss 0.254172\n",
      "iteration 4090: loss 0.254594\n",
      "iteration 4100: loss 0.254682\n",
      "iteration 4110: loss 0.254467\n",
      "iteration 4120: loss 0.254108\n",
      "iteration 4130: loss 0.253628\n",
      "iteration 4140: loss 0.252810\n",
      "iteration 4150: loss 0.252279\n",
      "iteration 4160: loss 0.252375\n",
      "iteration 4170: loss 0.252466\n",
      "iteration 4180: loss 0.252583\n",
      "iteration 4190: loss 0.252584\n",
      "iteration 4200: loss 0.252494\n",
      "iteration 4210: loss 0.252529\n",
      "iteration 4220: loss 0.252541\n",
      "iteration 4230: loss 0.252557\n",
      "iteration 4240: loss 0.252503\n",
      "iteration 4250: loss 0.252400\n",
      "iteration 4260: loss 0.252283\n",
      "iteration 4270: loss 0.252345\n",
      "iteration 4280: loss 0.252382\n",
      "iteration 4290: loss 0.252271\n",
      "iteration 4300: loss 0.252251\n",
      "iteration 4310: loss 0.252287\n",
      "iteration 4320: loss 0.252248\n",
      "iteration 4330: loss 0.252275\n",
      "iteration 4340: loss 0.252298\n",
      "iteration 4350: loss 0.252231\n",
      "iteration 4360: loss 0.252392\n",
      "iteration 4370: loss 0.252295\n",
      "iteration 4380: loss 0.252338\n",
      "iteration 4390: loss 0.252251\n",
      "iteration 4400: loss 0.252180\n",
      "iteration 4410: loss 0.252085\n",
      "iteration 4420: loss 0.251920\n",
      "iteration 4430: loss 0.251913\n",
      "iteration 4440: loss 0.251997\n",
      "iteration 4450: loss 0.251918\n",
      "iteration 4460: loss 0.251873\n",
      "iteration 4470: loss 0.251864\n",
      "iteration 4480: loss 0.251909\n",
      "iteration 4490: loss 0.251799\n",
      "iteration 4500: loss 0.251764\n",
      "iteration 4510: loss 0.251792\n",
      "iteration 4520: loss 0.251692\n",
      "iteration 4530: loss 0.251605\n",
      "iteration 4540: loss 0.251625\n",
      "iteration 4550: loss 0.251580\n",
      "iteration 4560: loss 0.251603\n",
      "iteration 4570: loss 0.251529\n",
      "iteration 4580: loss 0.251513\n",
      "iteration 4590: loss 0.251503\n",
      "iteration 4600: loss 0.251451\n",
      "iteration 4610: loss 0.251481\n",
      "iteration 4620: loss 0.251423\n",
      "iteration 4630: loss 0.251441\n",
      "iteration 4640: loss 0.251369\n",
      "iteration 4650: loss 0.251312\n",
      "iteration 4660: loss 0.251336\n",
      "iteration 4670: loss 0.251229\n",
      "iteration 4680: loss 0.251235\n",
      "iteration 4690: loss 0.251233\n",
      "iteration 4700: loss 0.251202\n",
      "iteration 4710: loss 0.251209\n",
      "iteration 4720: loss 0.251164\n",
      "iteration 4730: loss 0.251208\n",
      "iteration 4740: loss 0.251191\n",
      "iteration 4750: loss 0.251192\n",
      "iteration 4760: loss 0.251142\n",
      "iteration 4770: loss 0.251158\n",
      "iteration 4780: loss 0.251184\n",
      "iteration 4790: loss 0.251209\n",
      "iteration 4800: loss 0.251093\n",
      "iteration 4810: loss 0.251065\n",
      "iteration 4820: loss 0.251094\n",
      "iteration 4830: loss 0.251071\n",
      "iteration 4840: loss 0.251015\n",
      "iteration 4850: loss 0.251046\n",
      "iteration 4860: loss 0.251066\n",
      "iteration 4870: loss 0.251085\n",
      "iteration 4880: loss 0.251076\n",
      "iteration 4890: loss 0.250959\n",
      "iteration 4900: loss 0.250982\n",
      "iteration 4910: loss 0.250944\n",
      "iteration 4920: loss 0.250881\n",
      "iteration 4930: loss 0.250846\n",
      "iteration 4940: loss 0.250883\n",
      "iteration 4950: loss 0.250821\n",
      "iteration 4960: loss 0.250803\n",
      "iteration 4970: loss 0.250862\n",
      "iteration 4980: loss 0.250888\n",
      "iteration 4990: loss 0.250802\n",
      "iteration 5000: loss 0.250819\n",
      "iteration 5010: loss 0.250870\n",
      "iteration 5020: loss 0.250763\n",
      "iteration 5030: loss 0.250794\n",
      "iteration 5040: loss 0.250836\n",
      "iteration 5050: loss 0.250838\n",
      "iteration 5060: loss 0.250790\n",
      "iteration 5070: loss 0.250799\n",
      "iteration 5080: loss 0.250805\n",
      "iteration 5090: loss 0.250716\n",
      "iteration 5100: loss 0.250785\n",
      "iteration 5110: loss 0.250815\n",
      "iteration 5120: loss 0.250722\n",
      "iteration 5130: loss 0.250807\n",
      "iteration 5140: loss 0.250675\n",
      "iteration 5150: loss 0.250700\n",
      "iteration 5160: loss 0.250750\n",
      "iteration 5170: loss 0.250701\n",
      "iteration 5180: loss 0.250730\n",
      "iteration 5190: loss 0.250665\n",
      "iteration 5200: loss 0.250715\n",
      "iteration 5210: loss 0.250615\n",
      "iteration 5220: loss 0.250672\n",
      "iteration 5230: loss 0.250620\n",
      "iteration 5240: loss 0.250669\n",
      "iteration 5250: loss 0.250617\n",
      "iteration 5260: loss 0.250649\n",
      "iteration 5270: loss 0.250762\n",
      "iteration 5280: loss 0.250664\n",
      "iteration 5290: loss 0.250713\n",
      "iteration 5300: loss 0.250609\n",
      "iteration 5310: loss 0.250721\n",
      "iteration 5320: loss 0.250637\n",
      "iteration 5330: loss 0.250675\n",
      "iteration 5340: loss 0.250623\n",
      "iteration 5350: loss 0.250680\n",
      "iteration 5360: loss 0.250574\n",
      "iteration 5370: loss 0.250699\n",
      "iteration 5380: loss 0.250625\n",
      "iteration 5390: loss 0.250608\n",
      "iteration 5400: loss 0.250647\n",
      "iteration 5410: loss 0.250596\n",
      "iteration 5420: loss 0.250682\n",
      "iteration 5430: loss 0.250591\n",
      "iteration 5440: loss 0.250531\n",
      "iteration 5450: loss 0.250636\n",
      "iteration 5460: loss 0.250567\n",
      "iteration 5470: loss 0.250656\n",
      "iteration 5480: loss 0.250586\n",
      "iteration 5490: loss 0.250528\n",
      "iteration 5500: loss 0.250592\n",
      "iteration 5510: loss 0.250522\n",
      "iteration 5520: loss 0.250585\n",
      "iteration 5530: loss 0.250509\n",
      "iteration 5540: loss 0.250488\n",
      "iteration 5550: loss 0.250576\n",
      "iteration 5560: loss 0.250512\n",
      "iteration 5570: loss 0.250467\n",
      "iteration 5580: loss 0.250521\n",
      "iteration 5590: loss 0.250467\n",
      "iteration 5600: loss 0.250426\n",
      "iteration 5610: loss 0.250430\n",
      "iteration 5620: loss 0.250361\n",
      "iteration 5630: loss 0.250345\n",
      "iteration 5640: loss 0.250425\n",
      "iteration 5650: loss 0.250360\n",
      "iteration 5660: loss 0.250324\n",
      "iteration 5670: loss 0.250381\n",
      "iteration 5680: loss 0.250330\n",
      "iteration 5690: loss 0.250302\n",
      "iteration 5700: loss 0.250400\n",
      "iteration 5710: loss 0.250337\n",
      "iteration 5720: loss 0.250327\n",
      "iteration 5730: loss 0.250384\n",
      "iteration 5740: loss 0.250324\n",
      "iteration 5750: loss 0.250285\n",
      "iteration 5760: loss 0.250359\n",
      "iteration 5770: loss 0.250295\n",
      "iteration 5780: loss 0.250260\n",
      "iteration 5790: loss 0.250235\n",
      "iteration 5800: loss 0.250271\n",
      "iteration 5810: loss 0.250186\n",
      "iteration 5820: loss 0.250114\n",
      "iteration 5830: loss 0.250200\n",
      "iteration 5840: loss 0.250150\n",
      "iteration 5850: loss 0.250147\n",
      "iteration 5860: loss 0.250245\n",
      "iteration 5870: loss 0.250227\n",
      "iteration 5880: loss 0.250197\n",
      "iteration 5890: loss 0.250199\n",
      "iteration 5900: loss 0.250178\n",
      "iteration 5910: loss 0.250159\n",
      "iteration 5920: loss 0.250191\n",
      "iteration 5930: loss 0.250167\n",
      "iteration 5940: loss 0.250153\n",
      "iteration 5950: loss 0.250186\n",
      "iteration 5960: loss 0.250119\n",
      "iteration 5970: loss 0.250131\n",
      "iteration 5980: loss 0.250178\n",
      "iteration 5990: loss 0.250116\n",
      "iteration 6000: loss 0.250081\n",
      "iteration 6010: loss 0.250159\n",
      "iteration 6020: loss 0.250167\n",
      "iteration 6030: loss 0.250109\n",
      "iteration 6040: loss 0.250120\n",
      "iteration 6050: loss 0.250088\n",
      "iteration 6060: loss 0.250069\n",
      "iteration 6070: loss 0.250151\n",
      "iteration 6080: loss 0.250061\n",
      "iteration 6090: loss 0.250052\n",
      "iteration 6100: loss 0.250116\n",
      "iteration 6110: loss 0.250053\n",
      "iteration 6120: loss 0.250048\n",
      "iteration 6130: loss 0.250149\n",
      "iteration 6140: loss 0.250067\n",
      "iteration 6150: loss 0.250015\n",
      "iteration 6160: loss 0.250065\n",
      "iteration 6170: loss 0.250041\n",
      "iteration 6180: loss 0.250009\n",
      "iteration 6190: loss 0.250064\n",
      "iteration 6200: loss 0.250004\n",
      "iteration 6210: loss 0.249986\n",
      "iteration 6220: loss 0.249976\n",
      "iteration 6230: loss 0.250066\n",
      "iteration 6240: loss 0.249998\n",
      "iteration 6250: loss 0.249978\n",
      "iteration 6260: loss 0.250009\n",
      "iteration 6270: loss 0.249998\n",
      "iteration 6280: loss 0.249975\n",
      "iteration 6290: loss 0.249950\n",
      "iteration 6300: loss 0.250014\n",
      "iteration 6310: loss 0.249965\n",
      "iteration 6320: loss 0.249953\n",
      "iteration 6330: loss 0.250008\n",
      "iteration 6340: loss 0.249957\n",
      "iteration 6350: loss 0.249957\n",
      "iteration 6360: loss 0.249962\n",
      "iteration 6370: loss 0.249996\n",
      "iteration 6380: loss 0.249910\n",
      "iteration 6390: loss 0.249941\n",
      "iteration 6400: loss 0.249890\n",
      "iteration 6410: loss 0.249948\n",
      "iteration 6420: loss 0.249917\n",
      "iteration 6430: loss 0.249926\n",
      "iteration 6440: loss 0.249872\n",
      "iteration 6450: loss 0.249925\n",
      "iteration 6460: loss 0.249903\n",
      "iteration 6470: loss 0.249879\n",
      "iteration 6480: loss 0.249909\n",
      "iteration 6490: loss 0.249917\n",
      "iteration 6500: loss 0.249845\n",
      "iteration 6510: loss 0.249867\n",
      "iteration 6520: loss 0.249909\n",
      "iteration 6530: loss 0.249882\n",
      "iteration 6540: loss 0.249841\n",
      "iteration 6550: loss 0.249827\n",
      "iteration 6560: loss 0.249902\n",
      "iteration 6570: loss 0.249834\n",
      "iteration 6580: loss 0.249809\n",
      "iteration 6590: loss 0.249854\n",
      "iteration 6600: loss 0.249823\n",
      "iteration 6610: loss 0.249806\n",
      "iteration 6620: loss 0.249852\n",
      "iteration 6630: loss 0.249793\n",
      "iteration 6640: loss 0.249773\n",
      "iteration 6650: loss 0.249852\n",
      "iteration 6660: loss 0.249795\n",
      "iteration 6670: loss 0.249745\n",
      "iteration 6680: loss 0.249841\n",
      "iteration 6690: loss 0.249779\n",
      "iteration 6700: loss 0.249762\n",
      "iteration 6710: loss 0.249798\n",
      "iteration 6720: loss 0.249756\n",
      "iteration 6730: loss 0.249754\n",
      "iteration 6740: loss 0.249798\n",
      "iteration 6750: loss 0.249740\n",
      "iteration 6760: loss 0.249721\n",
      "iteration 6770: loss 0.249825\n",
      "iteration 6780: loss 0.249744\n",
      "iteration 6790: loss 0.249757\n",
      "iteration 6800: loss 0.249773\n",
      "iteration 6810: loss 0.249715\n",
      "iteration 6820: loss 0.249698\n",
      "iteration 6830: loss 0.249787\n",
      "iteration 6840: loss 0.249675\n",
      "iteration 6850: loss 0.249673\n",
      "iteration 6860: loss 0.249769\n",
      "iteration 6870: loss 0.249706\n",
      "iteration 6880: loss 0.249687\n",
      "iteration 6890: loss 0.249710\n",
      "iteration 6900: loss 0.249629\n",
      "iteration 6910: loss 0.249614\n",
      "iteration 6920: loss 0.249654\n",
      "iteration 6930: loss 0.249673\n",
      "iteration 6940: loss 0.249676\n",
      "iteration 6950: loss 0.249608\n",
      "iteration 6960: loss 0.249552\n",
      "iteration 6970: loss 0.249531\n",
      "iteration 6980: loss 0.249517\n",
      "iteration 6990: loss 0.249615\n",
      "iteration 7000: loss 0.249580\n",
      "iteration 7010: loss 0.249558\n",
      "iteration 7020: loss 0.249551\n",
      "iteration 7030: loss 0.249565\n",
      "iteration 7040: loss 0.249611\n",
      "iteration 7050: loss 0.249604\n",
      "iteration 7060: loss 0.249547\n",
      "iteration 7070: loss 0.249536\n",
      "iteration 7080: loss 0.249625\n",
      "iteration 7090: loss 0.249569\n",
      "iteration 7100: loss 0.249576\n",
      "iteration 7110: loss 0.249549\n",
      "iteration 7120: loss 0.249612\n",
      "iteration 7130: loss 0.249572\n",
      "iteration 7140: loss 0.249543\n",
      "iteration 7150: loss 0.249558\n",
      "iteration 7160: loss 0.249555\n",
      "iteration 7170: loss 0.249629\n",
      "iteration 7180: loss 0.249584\n",
      "iteration 7190: loss 0.249516\n",
      "iteration 7200: loss 0.249531\n",
      "iteration 7210: loss 0.249607\n",
      "iteration 7220: loss 0.249558\n",
      "iteration 7230: loss 0.249537\n",
      "iteration 7240: loss 0.249511\n",
      "iteration 7250: loss 0.249451\n",
      "iteration 7260: loss 0.249587\n",
      "iteration 7270: loss 0.249540\n",
      "iteration 7280: loss 0.249530\n",
      "iteration 7290: loss 0.249593\n",
      "iteration 7300: loss 0.249504\n",
      "iteration 7310: loss 0.249472\n",
      "iteration 7320: loss 0.249498\n",
      "iteration 7330: loss 0.249471\n",
      "iteration 7340: loss 0.249567\n",
      "iteration 7350: loss 0.249481\n",
      "iteration 7360: loss 0.249457\n",
      "iteration 7370: loss 0.249503\n",
      "iteration 7380: loss 0.249496\n",
      "iteration 7390: loss 0.249468\n",
      "iteration 7400: loss 0.249487\n",
      "iteration 7410: loss 0.249450\n",
      "iteration 7420: loss 0.249487\n",
      "iteration 7430: loss 0.249479\n",
      "iteration 7440: loss 0.249535\n",
      "iteration 7450: loss 0.249451\n",
      "iteration 7460: loss 0.249405\n",
      "iteration 7470: loss 0.249499\n",
      "iteration 7480: loss 0.249477\n",
      "iteration 7490: loss 0.249515\n",
      "iteration 7500: loss 0.249431\n",
      "iteration 7510: loss 0.249396\n",
      "iteration 7520: loss 0.249478\n",
      "iteration 7530: loss 0.249471\n",
      "iteration 7540: loss 0.249481\n",
      "iteration 7550: loss 0.249402\n",
      "iteration 7560: loss 0.249353\n",
      "iteration 7570: loss 0.249398\n",
      "iteration 7580: loss 0.249436\n",
      "iteration 7590: loss 0.249406\n",
      "iteration 7600: loss 0.249391\n",
      "iteration 7610: loss 0.249415\n",
      "iteration 7620: loss 0.249456\n",
      "iteration 7630: loss 0.249373\n",
      "iteration 7640: loss 0.249369\n",
      "iteration 7650: loss 0.249387\n",
      "iteration 7660: loss 0.249336\n",
      "iteration 7670: loss 0.249370\n",
      "iteration 7680: loss 0.249423\n",
      "iteration 7690: loss 0.249363\n",
      "iteration 7700: loss 0.249526\n",
      "iteration 7710: loss 0.250111\n",
      "iteration 7720: loss 0.250511\n",
      "iteration 7730: loss 0.250720\n",
      "iteration 7740: loss 0.250652\n",
      "iteration 7750: loss 0.250333\n",
      "iteration 7760: loss 0.250008\n",
      "iteration 7770: loss 0.249547\n",
      "iteration 7780: loss 0.249202\n",
      "iteration 7790: loss 0.249006\n",
      "iteration 7800: loss 0.248902\n",
      "iteration 7810: loss 0.249014\n",
      "iteration 7820: loss 0.249084\n",
      "iteration 7830: loss 0.249096\n",
      "iteration 7840: loss 0.249129\n",
      "iteration 7850: loss 0.249125\n",
      "iteration 7860: loss 0.249199\n",
      "iteration 7870: loss 0.249152\n",
      "iteration 7880: loss 0.249149\n",
      "iteration 7890: loss 0.249181\n",
      "iteration 7900: loss 0.249220\n",
      "iteration 7910: loss 0.249392\n",
      "iteration 7920: loss 0.249795\n",
      "iteration 7930: loss 0.250024\n",
      "iteration 7940: loss 0.250088\n",
      "iteration 7950: loss 0.250029\n",
      "iteration 7960: loss 0.249879\n",
      "iteration 7970: loss 0.249806\n",
      "iteration 7980: loss 0.249695\n",
      "iteration 7990: loss 0.249567\n",
      "iteration 8000: loss 0.249347\n",
      "iteration 8010: loss 0.249182\n",
      "iteration 8020: loss 0.249061\n",
      "iteration 8030: loss 0.248988\n",
      "iteration 8040: loss 0.249051\n",
      "iteration 8050: loss 0.249100\n",
      "iteration 8060: loss 0.249197\n",
      "iteration 8070: loss 0.249516\n",
      "iteration 8080: loss 0.249692\n",
      "iteration 8090: loss 0.249791\n",
      "iteration 8100: loss 0.249742\n",
      "iteration 8110: loss 0.249687\n",
      "iteration 8120: loss 0.249586\n",
      "iteration 8130: loss 0.249564\n",
      "iteration 8140: loss 0.249450\n",
      "iteration 8150: loss 0.249283\n",
      "iteration 8160: loss 0.249148\n",
      "iteration 8170: loss 0.249129\n",
      "iteration 8180: loss 0.249311\n",
      "iteration 8190: loss 0.249507\n",
      "iteration 8200: loss 0.249544\n",
      "iteration 8210: loss 0.249556\n",
      "iteration 8220: loss 0.249527\n",
      "iteration 8230: loss 0.249411\n",
      "iteration 8240: loss 0.249284\n",
      "iteration 8250: loss 0.249181\n",
      "iteration 8260: loss 0.249097\n",
      "iteration 8270: loss 0.249213\n",
      "iteration 8280: loss 0.249354\n",
      "iteration 8290: loss 0.249506\n",
      "iteration 8300: loss 0.249529\n",
      "iteration 8310: loss 0.249490\n",
      "iteration 8320: loss 0.249463\n",
      "iteration 8330: loss 0.249352\n",
      "iteration 8340: loss 0.249185\n",
      "iteration 8350: loss 0.248920\n",
      "iteration 8360: loss 0.248751\n",
      "iteration 8370: loss 0.248680\n",
      "iteration 8380: loss 0.248761\n",
      "iteration 8390: loss 0.248773\n",
      "iteration 8400: loss 0.248805\n",
      "iteration 8410: loss 0.248737\n",
      "iteration 8420: loss 0.248745\n",
      "iteration 8430: loss 0.248761\n",
      "iteration 8440: loss 0.248773\n",
      "iteration 8450: loss 0.248912\n",
      "iteration 8460: loss 0.249188\n",
      "iteration 8470: loss 0.249328\n",
      "iteration 8480: loss 0.249375\n",
      "iteration 8490: loss 0.249237\n",
      "iteration 8500: loss 0.249005\n",
      "iteration 8510: loss 0.248809\n",
      "iteration 8520: loss 0.248664\n",
      "iteration 8530: loss 0.248713\n",
      "iteration 8540: loss 0.248706\n",
      "iteration 8550: loss 0.248705\n",
      "iteration 8560: loss 0.248715\n",
      "iteration 8570: loss 0.248707\n",
      "iteration 8580: loss 0.248642\n",
      "iteration 8590: loss 0.248653\n",
      "iteration 8600: loss 0.248745\n",
      "iteration 8610: loss 0.248956\n",
      "iteration 8620: loss 0.249102\n",
      "iteration 8630: loss 0.249159\n",
      "iteration 8640: loss 0.249114\n",
      "iteration 8650: loss 0.248984\n",
      "iteration 8660: loss 0.248847\n",
      "iteration 8670: loss 0.248706\n",
      "iteration 8680: loss 0.248617\n",
      "iteration 8690: loss 0.248439\n",
      "iteration 8700: loss 0.248276\n",
      "iteration 8710: loss 0.248218\n",
      "iteration 8720: loss 0.248238\n",
      "iteration 8730: loss 0.248397\n",
      "iteration 8740: loss 0.248469\n",
      "iteration 8750: loss 0.248618\n",
      "iteration 8760: loss 0.248823\n",
      "iteration 8770: loss 0.249014\n",
      "iteration 8780: loss 0.249085\n",
      "iteration 8790: loss 0.249058\n",
      "iteration 8800: loss 0.248843\n",
      "iteration 8810: loss 0.248663\n",
      "iteration 8820: loss 0.248617\n",
      "iteration 8830: loss 0.248541\n",
      "iteration 8840: loss 0.248614\n",
      "iteration 8850: loss 0.248739\n",
      "iteration 8860: loss 0.248917\n",
      "iteration 8870: loss 0.248971\n",
      "iteration 8880: loss 0.248855\n",
      "iteration 8890: loss 0.248681\n",
      "iteration 8900: loss 0.248538\n",
      "iteration 8910: loss 0.248475\n",
      "iteration 8920: loss 0.248206\n",
      "iteration 8930: loss 0.248095\n",
      "iteration 8940: loss 0.247924\n",
      "iteration 8950: loss 0.247858\n",
      "iteration 8960: loss 0.247890\n",
      "iteration 8970: loss 0.247991\n",
      "iteration 8980: loss 0.248207\n",
      "iteration 8990: loss 0.248362\n",
      "iteration 9000: loss 0.248279\n",
      "iteration 9010: loss 0.248119\n",
      "iteration 9020: loss 0.248049\n",
      "iteration 9030: loss 0.248012\n",
      "iteration 9040: loss 0.247999\n",
      "iteration 9050: loss 0.247982\n",
      "iteration 9060: loss 0.247990\n",
      "iteration 9070: loss 0.248079\n",
      "iteration 9080: loss 0.248190\n",
      "iteration 9090: loss 0.248279\n",
      "iteration 9100: loss 0.248325\n",
      "iteration 9110: loss 0.248483\n",
      "iteration 9120: loss 0.248785\n",
      "iteration 9130: loss 0.248970\n",
      "iteration 9140: loss 0.249040\n",
      "iteration 9150: loss 0.249040\n",
      "iteration 9160: loss 0.248955\n",
      "iteration 9170: loss 0.248739\n",
      "iteration 9180: loss 0.248477\n",
      "iteration 9190: loss 0.248332\n",
      "iteration 9200: loss 0.248222\n",
      "iteration 9210: loss 0.247873\n",
      "iteration 9220: loss 0.247807\n",
      "iteration 9230: loss 0.247853\n",
      "iteration 9240: loss 0.247887\n",
      "iteration 9250: loss 0.247952\n",
      "iteration 9260: loss 0.248069\n",
      "iteration 9270: loss 0.248247\n",
      "iteration 9280: loss 0.248363\n",
      "iteration 9290: loss 0.248417\n",
      "iteration 9300: loss 0.248731\n",
      "iteration 9310: loss 0.248890\n",
      "iteration 9320: loss 0.248884\n",
      "iteration 9330: loss 0.248862\n",
      "iteration 9340: loss 0.248803\n",
      "iteration 9350: loss 0.248696\n",
      "iteration 9360: loss 0.248562\n",
      "iteration 9370: loss 0.248405\n",
      "iteration 9380: loss 0.248471\n",
      "iteration 9390: loss 0.248652\n",
      "iteration 9400: loss 0.248724\n",
      "iteration 9410: loss 0.248728\n",
      "iteration 9420: loss 0.248639\n",
      "iteration 9430: loss 0.248543\n",
      "iteration 9440: loss 0.248367\n",
      "iteration 9450: loss 0.248420\n",
      "iteration 9460: loss 0.248598\n",
      "iteration 9470: loss 0.248657\n",
      "iteration 9480: loss 0.248671\n",
      "iteration 9490: loss 0.248617\n",
      "iteration 9500: loss 0.248481\n",
      "iteration 9510: loss 0.248415\n",
      "iteration 9520: loss 0.248464\n",
      "iteration 9530: loss 0.248585\n",
      "iteration 9540: loss 0.248634\n",
      "iteration 9550: loss 0.248623\n",
      "iteration 9560: loss 0.248491\n",
      "iteration 9570: loss 0.248419\n",
      "iteration 9580: loss 0.248398\n",
      "iteration 9590: loss 0.248499\n",
      "iteration 9600: loss 0.248554\n",
      "iteration 9610: loss 0.248576\n",
      "iteration 9620: loss 0.248477\n",
      "iteration 9630: loss 0.248384\n",
      "iteration 9640: loss 0.248357\n",
      "iteration 9650: loss 0.248445\n",
      "iteration 9660: loss 0.248539\n",
      "iteration 9670: loss 0.248521\n",
      "iteration 9680: loss 0.248539\n",
      "iteration 9690: loss 0.248397\n",
      "iteration 9700: loss 0.248310\n",
      "iteration 9710: loss 0.248285\n",
      "iteration 9720: loss 0.248429\n",
      "iteration 9730: loss 0.248509\n",
      "iteration 9740: loss 0.248547\n",
      "iteration 9750: loss 0.248490\n",
      "iteration 9760: loss 0.248415\n",
      "iteration 9770: loss 0.248320\n",
      "iteration 9780: loss 0.248354\n",
      "iteration 9790: loss 0.248455\n",
      "iteration 9800: loss 0.248516\n",
      "iteration 9810: loss 0.248455\n",
      "iteration 9820: loss 0.248428\n",
      "iteration 9830: loss 0.248384\n",
      "iteration 9840: loss 0.248308\n",
      "iteration 9850: loss 0.248350\n",
      "iteration 9860: loss 0.248452\n",
      "iteration 9870: loss 0.248471\n",
      "iteration 9880: loss 0.248469\n",
      "iteration 9890: loss 0.248308\n",
      "iteration 9900: loss 0.248254\n",
      "iteration 9910: loss 0.248312\n",
      "iteration 9920: loss 0.248417\n",
      "iteration 9930: loss 0.248458\n",
      "iteration 9940: loss 0.248465\n",
      "iteration 9950: loss 0.248454\n",
      "iteration 9960: loss 0.248373\n",
      "iteration 9970: loss 0.248262\n",
      "iteration 9980: loss 0.248206\n",
      "iteration 9990: loss 0.247776\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    hidden_layer = np.maximum(0,np.dot(X, W1) + b1)\n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "#   hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\n",
    "#   scores = np.dot(hidden_layer, W2) + b2\n",
    "\n",
    "    num_examples = X.shape[0]\n",
    "    # get unnormalized probabilities\n",
    "\n",
    "    exp_scores = np.exp(scores)\n",
    "    # normalize them for each example\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "    # # compute the loss: average cross-entropy loss and regularization    \n",
    "    data_loss = np.sum(corect_logprobs)/num_examples\n",
    "    #reg_loss = 0.5*reg*np.sum(W*W)\n",
    "    reg_loss = 0.5*reg*np.sum(W1*W1) + 0.5*reg*np.sum(W2*W2)\n",
    "    loss = data_loss + reg_loss\n",
    "\n",
    "#     exp_scores = np.exp(scores)\n",
    "#     probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "#     # compute the loss: average cross-entropy loss and regularization\n",
    "#     corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "#     data_loss = np.sum(corect_logprobs)/num_examples\n",
    "#     reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "#     loss = data_loss + reg_loss\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, loss))\n",
    "#     dscores = probs\n",
    "#     dscores[range(num_examples),y] -= 1\n",
    "#     dscores /= num_examples\n",
    "\n",
    "#     # backpropate the gradient to the parameters\n",
    "#     # first backprop into parameters W2 and b2\n",
    "#     dW2 = np.dot(hidden_layer.T, dscores)\n",
    "#     db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "#     # next backprop into hidden layer\n",
    "#     dhidden = np.dot(dscores, W2.T)\n",
    "#     # backprop the ReLU non-linearity\n",
    "#     dhidden[hidden_layer <= 0] = 0\n",
    "#     # finally into W,b\n",
    "#     dW = np.dot(X.T, dhidden)\n",
    "#     db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),y] -= 1\n",
    "    dscores /= num_examples\n",
    "\n",
    "    dW2 = np.dot(hidden_layer.T, dscores)\n",
    "    #dW2 += reg*W2 \n",
    "    db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "    dhidden = np.dot(dscores, W2.T)\n",
    "    dhidden[hidden_layer <= 0] = 0\n",
    "    dW1 = np.dot(X.T, dhidden)\n",
    "    db1 = np.sum(dhidden, axis=0, keepdims=True)\n",
    "\n",
    "#     dW2 += reg * W2\n",
    "#     dW += reg * W\n",
    "\n",
    "#     # perform a parameter update\n",
    "#     W += -step_size * dW\n",
    "#     b += -step_size * db\n",
    "#     W2 += -step_size * dW2\n",
    "#     b2 += -step_size * db2\n",
    "\n",
    "    dW2 += reg*W2 \n",
    "    dW1 += reg*W1 # don't forget the regularization gradient\n",
    "\n",
    "    W1 += -step_size * dW1\n",
    "    b1 += -step_size * db1\n",
    "    W2 += -step_size * dW2\n",
    "    b2 += -step_size * db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# evaluate training set accuracy\n",
    "hidden_layer = np.maximum(0, np.dot(X, W1) + b1)\n",
    "scores = np.dot(hidden_layer, W2) + b2\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print('training accuracy: %.2f' % (np.mean(predicted_class == y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
